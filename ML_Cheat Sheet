General Jargons:
1. Data Mining:Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.(Source: Wikipedia)
2. Machine Learning:Machine Learning is a discipline within artificial intelligence that is focused on using data (or interactive experience) to build intelligent systems
3. Artificial Intelligence:Artificial Intelligence is a collection of problems and methods related to making computers behave intelligently and solve complex problems
4. Data Science:Data Science involves skills and concepts from several different areas, including statistics, machine learning, and visualization
5. Big Data:Big data is a term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with
6. Information Retrieval:it is the activity of obtaining information system resources relevant to an information need from a collection of information resources
7. Data Analytics:Data analytics (DA) is the process of examining data sets in order to draw conclusions about the information they contain, increasingly with the aid of specialized systems and software. 
8. Deep Learning:Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.
9. Artificial Neural Networks: Its a framework for different Machine learning algorithms to work together and process complex data inputs.
10. How to choose number of clusters in Kmeans?
	- WCSS(Within Cluster Sum of Squares) also called 'Inertia'
	- Elbow method - Look for the point where the drop in WCSS drop changes from big value to relatively small value.
11. Constraining a model to make it simpler and reduce the risk of overfitting is called regularization
12. The error rate on new cases is called the generalization error (or out-of-sample error)
13. If the training error is low (i.e., your model makes few mistakes on the training set) but the generalization error is high, it means that your model is overfitting the training data.


ML vs AI:

ARTIFICIAL INTELLIGENCE														MACHINE LEARNING
1. AI stands for Artificial intelligence, where intelligence is defined acquisition     1. ML stands for Machine Learning which is defined as the acquisition of knowledge or skill
of knowledge intelligence is defined as a ability to acquire and apply knowledge.	
2. The aim is to increase chance of success and not accuracy.				2. The aim is to increase accuracy, but it does not care about success
3. It work as a computer program that does smart work					3. It is a simple concept machine takes data and learn from data.
4. The goal is to simulate natural intelligence to solve complex problem		4. The goal is to learn from data on certain task to maximize the performance of machine on this task.
5. AI is decision making.								5. ML allows system to learn new things from data.
6. It leads to develop a system to mimic human to respond behave in a circumstances.	6. It involves in creating self learning algorithms.
7. AI will go for finding the optimal solution.						7. ML will go for only solution for that whether it is optimal or not.
8. AI leads to intelligence or wisdom.							8. ML leads to knowledge.



1. Keras - Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.
2. TensorFlow - TensorFlow™ is an open source software library for high performance numerical computation

Label(Y) - lable is a true thing which we are predicting ( example - spam or not spam)
Features(x) - features are input variables describing out data ( example - words in email , to and from address etc)
Examples - Example is an instance of data
		Two categories of examples:	
			labled: A labeled example includes both feature(s) and the label.
			unlabled: An unlabeled example contains features but not the label
Models - A model defines the relationship between features and label
Training - Training means creating or learning the model.That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label
Inference - Inference means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y').
Loss - Loss or squared error or L2 Loss is the square of difference between actual value(lable) and the predicted value(prediction). ie (y-y')2
empirical risk minimization - its a process to minimize the loss
Mean square error (MSE) - is the average squared loss per example over the whole dataset
Stochastic Gradient Descent: one example at a time
Mini-Batch Gradient Descent: batches of 10-1000


3. Feature scaling is also known as Data Normalization. Its a technique used to standardize the range of independent variables or features of data. 
4. Categorical Variables: Its a variable which can take on one of the limited and usually fixed number of possible values.
5. Why is Data Normalization or Feature Scaling required?
	Ans: parameters should have the same scale for a fair comparison between them.
	Two methods are usually well known for rescaling data. Normalization, which scales all numeric variables in the range [0,1]. 
	x' = x-min(x)/max(x)-min(x-min)
	On the other hand, you can use standardization on your data set. It will then transform it to have zero mean and unit variance, for example using the equation below:
	x' = x - mean(x)/standard_devation(x)
	
	Both of these techniques have their drawbacks. If you have outliers in your data set, normalizing your data will certainly scale the “normal” data to a very small interval. And generally, most of data sets have outliers. When using standardization, your new data aren’t bounded (unlike normalization).
6. Steps in preprocessing using python:
	1. importing the necessary libraries
	2. Importing the data set using these libraries ex: with Pandas
	3. handling missing data
	4. Encoding categorical Variables
	5. splitting the data set into Training and test data
	6. Optionally doing Data Normalization or feature scaling
	
7. Different Regression Models:
	1. Simple Linear Regression
	2. Multiple Linear Regression
	3. Polynomial Regression
	4. Support Vector for Regression (SVR)
	5. Decision Tree Classification
	6. Random Forest Classification
	
7.1 Simple Linear Regression:
	y = b0+b1x where X is the independent variable and Y is the dependent Variables. 
		b0 is the Y-Intercept and b1 is the slope of the straight line.
	Typically uses the Ordinary least square method. Its a technique of minimizing the sum of squares of the differences between the Observed dependent variable with those predicted by the model or linear function.
		


